{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": {
        "input": [
            "\"2025/07/28\":\"2025年7月28日是一个具体的日期，标志着一次关于大型语言模型的会议活动。\"",
            "\"SPEAKER-1\":\"speaker-1是会议中的发言人，负责介绍大型语言模型综合报告的内容。\"",
            "\"SPEAKER-2\":**SPEAKER-2** 是会议中的发言人，与 **SPEAKER-1** 一起参与讨论大型语言模型的相关话题。在时间 **122.65s - 260.29s** 期间，**SPEAKER-2** 发言，讨论了大型语言模型的原理。",
            "\"SPEAKER-3\":**SPEAKER-3** 是会议中的发言人，与其他两位发言人共同参与关于大型语言模型的讨论。作为提供关于大型语言模型发展历史的演讲者，SPEAKER-3 的发言时间从 260.29 秒到 413.26 秒，内容涵盖了该领域的发展历程。",
            "\"人工智能\":\"人工智能是大型语言模型发展的领域，涉及研究人员、开发人员和公众的想象力。\"",
            "\"大型语言模型综合报告\":\"大型语言模型综合报告是2025年7月28日会议中的一个事件，旨在提供关于大型语言模型的全面概述。\"",
            "\"大型语言模型\":大型语言模型是一种专门用于理解和生成人类语言的人工智能模型，具有大规模数据训练和参数量大的特点。它属于当前自然语言处理领域的重要组织或项目，代表了具有数十亿甚至数万亿个参数的先进模型。",
            "\"LLM\":**LLM（大语言模型）** 是一种基于深度学习架构（如Transformer网络）的人工智能模型，能够处理文本等顺序数据。它是在大量文本和代码数据集上训练的AI，具备生成文本、翻译语言、编写各种创意内容以及以信息丰富的方式回答问题的能力。LLM被广泛应用于多个领域，包括代码生成、文本摘要、情绪分析、教育、研究和医疗保健。\n\n在教育领域，LLM用于创建个性化的学习体验、提供学生写作反馈并生成教育内容。在研究领域，它被用来帮助研究人员分析学术论文、识别相关信息并产生假设。在医疗保健领域，LLM被探索用于医疗诊断协助、药物发现和患者沟通等任务。\n\n综上所述，LLM是一种功能强大且多用途的大型语言模型，能够跨多个领域提供智能化的服务和支持。",
            "\"深度学习架构\":实体：“深度学习架构”\n\n描述：深度学习架构是参数模型的基础，尤其在大型语言模型中扮演关键角色。最著名的深度学习架构是transformer网络，它在处理顺序数据（如文本）方面表现出色，是当前自然语言处理任务中的核心技术。",
            "\"TRANSFORMER网络\":**TRANSFORMER网络**是一种深度学习架构，被证明在处理文本等顺序数据方面特别有效。它不仅在自然语言处理任务中表现出色，还作为大型语言模型的核心组成部分，为现代人工智能技术的发展提供了重要支撑。",
            "\"人类语言\":\"人类语言是大型语言模型理解和生成的对象，模型通过大量文本和代码学习复现人类语言的复杂性。\"",
            "\"参数模型\":\"参数模型是指基于深度学习架构（如transformer网络）的机器学习模型，用于在训练期间学习内部变量的数量。它在处理文本等顺序数据方面表现出色。\"",
            "\"文本\":\"文本是LLM处理的输入数据类型，用于训练模型以学习语言模式和生成文本。\"",
            "\"代码\":\"代码是LLM处理的输入数据类型，用于训练模型以学习编程语言和生成代码相关内容。\"",
            "\"书籍\":\"书籍是LLM训练数据的来源之一，提供文本内容以帮助模型理解语言结构。\"",
            "\"文章\":\"文章是LLM训练数据的来源之一，提供文本内容以帮助模型理解语言结构。\""
        ]
    }
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": {
        "input": [
            "\"网站\":\"网站是LLM训练数据的来源之一，提供文本内容以帮助模型理解语言结构。\"",
            "\"代码存储库\":\"代码存储库是LLM训练数据的来源之一，提供代码内容以帮助模型学习编程语言和生成代码相关内容。\"",
            "\"TRANSFORMER架构\":\"transformer架构是LLM的核心技术，于2017年推出，使模型能够有效捕获文本中的长距离依赖关系，彻底改变了自然语言处理。\"",
            "\"TRANSFORMER\":**Transformer** 是2017年推出的架构，标志着LLM历史上的一个分水岭。该架构因其处理远程依赖关系和并行处理能力，对自然语言处理领域产生了深远影响，显著提升了模型对文本中长距离依赖关系的捕捉能力，从而引发了NLP领域的范式转变。",
            "\"SELF-ATTENTION\":\"Self-attention是Transformer架构中的核心机制，使模型在处理每个单词时能够权衡输入序列中不同单词的重要性。\"",
            "\"自然语言处理\":自然语言处理（Natural Language Processing，NLP）是一个长期研究和创新的领域，代表了NLP领域数十年的研究和创新历程。它涉及机器使用自然语言与人类交互的潜力，并且是Transformer架构带来的重大变革领域，其效果超越了传统的递归神经网络RNN架构。自然语言处理涵盖了多个任务，包括问答、摘要、翻译等，这些任务通常需要模型被微调以优化性能。",
            "\"编码器\":编码器是“编码器”这一实体所属的Transformer模型中的关键组成部分，其主要职责是处理输入序列并生成其语义表示。该实体在Transformer模型中承担着将原始输入转换为具有语义信息的向量表示的功能，这一过程通过处理输入序列并创建其含义的表示形式来实现。所有描述均一致，无矛盾之处。因此，“编码器”在Transformer模型中负责将输入序列转化为语义丰富的表示形式，是模型理解输入内容的核心组件之一。",
            "\"解码器\":解码器是Transformer模型中的一个关键组成部分，主要用于生成输出序列。作为模型中占主导地位的部分，解码器负责利用编码器生成的表示形式，通过一系列变换和预测机制，逐步生成目标序列。在生成模型中，解码器承担着核心作用，确保输出序列的连贯性和准确性。",
            "\"递归神经网络RNN\":",
            "\"TRANSFORMER模型\":\"transformer模型是用于自然语言处理的架构，由编码器和解码器组成，广泛应用于各种语言任务中。\"",
            "\"自我监督学习\":\"自我监督学习是一种机器学习技术，用于在海量数据集上预训练模型，使其能够预测缺失的单词或下一个单词。\"\"自我监督学习是一种机器学习方法，用于在海量数据集上预训练模型，使其能够预测缺失的单词或下一个单词。\"",
            "\"微调\":微调是预训练后对大型语言模型（LLM）进行进一步优化的过程，针对特定任务使用领域特定的数据进行训练。该过程通常采用较小的特定领域数据集进行调整，以提升模型在目标任务上的性能。微调旨在使模型更好地适应特定应用场景，同时保持其在广泛语言理解上的通用能力。",
            "\"词\":\"词是语言的基本单位，涉及语言结构、语法语义以及训练数据中的世界知识。\"",
            "\"模型\":\"模型是经过预训练和微调的大型语言模型，用于处理各种自然语言处理任务。\"",
            "\"预训练\":\"预训练是模型在大规模数据上进行初始学习的过程，为后续微调打下基础。\"",
            "\"问答\":\"问答是通过在问题数据集及其答案上训练模型来优化的自然语言处理任务。\""
        ]
    }
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": {
        "input": [
            "\"文本摘要\":文本摘要是“文殊意坚”的一项功能，可以自动将冗长的文档压缩成简洁的摘要，帮助用户节省时间。作为一种通过微调模型来提升的自然语言处理任务，文本摘要体现了深度学习技术在信息浓缩方面的应用。",
            "\"翻译\":\"翻译是另一种通过微调模型来优化的自然语言处理任务。\"",
            "\"大型数据级\":\"大型数据级指的是模型训练所依赖的大量数据规模。\"",
            "\"参数数量\":\"参数数量是衡量模型规模的重要指标，影响模型的性能和能力。\"",
            "\"语法语义\":\"语法语义是模型处理语言规则和意义的核心能力，影响对文本的理解。\"",
            "\"训练数据\":\"训练数据是模型学习语言模式和世界知识的来源，影响其性能和泛化能力。\"",
            "\"计算能力\":\"计算能力是推动模型开发和训练的重要因素，与参数数量和数据规模相关。\"",
            "\"语言结构\":",
            "\"ELLIA\":\"ellia是1966年开发的一个系统，能够从输入中识别关键字并使用预编程的答案进行响应。\"",
            "\"RIN\":RIN是一种循环神经网络，用于处理文本等顺序数据，允许模型捕获句子中单词之间的依赖关系。作为循环神经网络的一种，RIN特别指的是长短期记忆LSTM网络，标志着更复杂的语言模型的发展。",
            "\"LSTM\":\"LSTM是RIN的一种类型，特别擅长处理长短期记忆问题。\"",
            "\"BERT\":BERT是由Direction开发的基于Transformer的模型，用于自然语言处理，能够生成高质量的文本表示。该模型利用Transformer架构，在自然语言处理任务中表现出色，尤其在文本表示学习方面具有显著优势。",
            "\"GPT\":GPT是生成式预训练Transformer的模型，用于自然语言处理。GPT是由OpenAI开发的模型系列，对普及大型语言模型（LLM）有重要影响，展示了在大规模文本语料库上预训练大型Transformer模型的能力。",
            "\"QUEEN\":\"queen是一个在向量空间中与king等词更接近的实体，可能指代某个特定人物或概念。\"",
            "\"VECTOR SPACE\":\"vector space是用于表示词语嵌入向量的空间，用于捕捉词语之间的语义关系。\"",
            "\"LONG SHORT-TERM MEMORY\":\"long short-term memory是LSTM的另一种称呼，是一种循环神经网络结构。\""
        ]
    }
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": {
        "input": [
            "\"NATURAL LANGUAGE PROCESSING\":\"natural language processing是自然语言处理领域的一个事件，标志着基于转换器的模型的发展。\"",
            "\"KING\":",
            "\"GPT2\":\"GPT2是GPT系列的延续，展示了即使没有明确微调，大型模型也能生成连贯且富有创意的文本的能力。\"",
            "\"GPT3\":GPT3是2020年推出的大型语言模型，属于GPT系列的最新版本。它拥有17501个参数，并在文本生成、理解和推理方面展现了卓越的能力，使其成为当时最先进的自然语言处理模型之一。",
            "\"大型模型\":\"大型模型是指一系列具有强大文本生成和理解能力的AI模型，包括GPT3、Google Lambda、Meta系列等，这些模型在规模和功能上不断进步。\"",
            "\"GOOGLE LAMBDA\":\"Google Lambda是谷歌开发的一种大型模型，与GPT3和其他模型一起推动了AI领域的发展。\"",
            "\"META系列\":\"Meta系列是指由Meta公司开发的一系列大型模型，这些模型在文本生成和理解方面表现出色。\"",
            "\"训练技术和数据管理\":",
            "\"的进步\":\"的进步是指持续推动快速发展的领域所发生的积极变化和进展。\")<|COMPLETE|>(\"entity\"",
            "\"代码生成\":\"代码生成是LLM在开发人员工作中的一项应用，能够根据自然语言描述生成代码片段、函数甚至整个程序。\"",
            "\"情绪分析\":\"情绪分析是LLM的一项应用，能够分析文本以确定情感语气或情绪，有助于了解客户反馈和市场趋势。\"",
            "\"教育\":\"教育是LLM的应用领域，用于创建个性化的学习体验，提供学生写作的反馈，并生成教育内容。\"",
            "\"研究\":\"研究是LLM的一项应用，帮助研究人员分析大量学术论文、识别相关信息并产生假设。\"",
            "\"医疗保健\":\"医疗保健是LLM正在探索的应用领域，用于医疗相关任务。\"",
            "\"学习体验\":",
            "\"学生写作反馈\":\"学生写作反馈是LLM提供的对学生作品的评估和建议。\""
        ]
    }
}
{
    "type": "error",
    "data": "Error Invoking LLM",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": {
        "input": [
            "\"教育内容生成\":\"教育内容生成是LLM生成的用于教学的材料。\"",
            "\"学术论文分析\":\"学术论文分析是LLM帮助研究人员进行的学术文献处理活动。\"",
            "\"信息识别\":\"信息识别是LLM在学术论文中发现和提取关键信息的过程。\"",
            "\"假设生成\":\"假设生成是LLM在学术研究中提出的新想法或理论的过程。\"",
            "\"医疗诊断协助\":\"医疗诊断协助是LLM在医疗领域提供的辅助医生诊断的服务。\"",
            "\"药物发现\":\"药物发现是LLM在医药研发中帮助发现新药的过程。\"",
            "\"患者沟通\":\"患者沟通是LLM在医疗保健中与患者进行交流的服务。\""
        ]
    }
}
{
    "type": "error",
    "data": "Error executing verb \"text_embed\" in create_final_entities: model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 415, in _execute_verb\n    result = await result\n             ^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\text_embed.py\", line 105, in text_embed\n    return await _text_embed_in_memory(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\text_embed.py\", line 130, in _text_embed_in_memory\n    result = await strategy_exec(texts, callbacks, cache, strategy_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 62, in run\n    embeddings = await _execute(llm, text_batches, ticker, semaphore)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 106, in _execute\n    results = await asyncio.gather(*futures)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 100, in embed\n    chunk_embeddings = await llm(chunk)\n                       ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": null
}
{
    "type": "error",
    "data": "Error running pipeline!",
    "stack": "Traceback (most recent call last):\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\run\\run.py\", line 227, in run_pipeline\n    result = await _process_workflow(\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\run\\workflow.py\", line 91, in _process_workflow\n    result = await workflow.run(context, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 369, in run\n    timing = await self._execute_verb(node, context, callbacks)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\datashaper\\workflow\\workflow.py\", line 415, in _execute_verb\n    result = await result\n             ^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\text_embed.py\", line 105, in text_embed\n    return await _text_embed_in_memory(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\text_embed.py\", line 130, in _text_embed_in_memory\n    result = await strategy_exec(texts, callbacks, cache, strategy_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 62, in run\n    embeddings = await _execute(llm, text_batches, ticker, semaphore)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 106, in _execute\n    results = await asyncio.gather(*futures)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\index\\verbs\\text\\embed\\strategies\\openai.py\", line 100, in embed\n    chunk_embeddings = await llm(chunk)\n                       ^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\caching_llm.py\", line 96, in __call__\n    result = await self._delegate(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 177, in __call__\n    result, start = await execute_with_retry()\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 159, in execute_with_retry\n    async for attempt in retryer:\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 166, in __anext__\n    do = await self.iter(retry_state=self._retry_state)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n    result = await action(retry_state)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\tenacity\\__init__.py\", line 400, in <lambda>\n    self._add_action_func(lambda rs: rs.outcome.result())\n                                     ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n    return self.__get_result()\n           ^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n    raise self._exception\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 165, in execute_with_retry\n    return await do_attempt(), start\n           ^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\rate_limiting_llm.py\", line 147, in do_attempt\n    return await self._delegate(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 50, in __call__\n    return await self._invoke(input, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\base\\base_llm.py\", line 54, in _invoke\n    output = await self._execute_llm(input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\LingQin\\ZHTLLM_UI_6\\ZHTLLM\\graphrag_online\\graphrag\\llm\\openai\\openai_embeddings_llm.py\", line 67, in _execute_llm\n    embedding = ollama.embeddings(model=self.configuration.model, prompt=inp)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 390, in embeddings\n    return self._request(\n           ^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 180, in _request\n    return cls(**self._request_raw(*args, **kwargs).json())\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\wisemodel-aipc-06-e\\anaconda3\\envs\\ZHTLLM_2\\Lib\\site-packages\\ollama\\_client.py\", line 124, in _request_raw\n    raise ResponseError(e.response.text, e.response.status_code) from None\nollama._types.ResponseError: model \"qwen:4b\" not found, try pulling it first (status code: 404)\n",
    "source": "model \"qwen:4b\" not found, try pulling it first (status code: 404)",
    "details": null
}
